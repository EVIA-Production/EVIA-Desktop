
import { useState, useEffect, useCallback } from 'react';
import { useToast } from '@/hooks/use-toast';
import { getWebSocketInstance, closeWebSocketInstance } from '@/services/websocketService';

export const useRecording = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [suggestion, setSuggestion] = useState('');
  const [isConnected, setIsConnected] = useState(false);
  const { toast } = useToast();
  
  const addDebugLog = (message: string, setDebugLog: React.Dispatch<React.SetStateAction<string[]>>) => {
    setDebugLog(prev => [...prev, `[${new Date().toISOString()}] ${message}`]);
    console.log(`DEBUG: ${message}`);
  };

  // Handle WebSocket messages
  const handleWebSocketMessage = useCallback((message: any) => {
    console.log('Received WebSocket message:', message);
    
    switch (message.type) {
      case 'transcript_segment':
        const { text, speaker, is_final } = message.content || {};
        // Update transcript UI
        setTranscript(prev => prev + (speaker ? `Speaker ${speaker}: ` : '') + text + '\n');
        break;
      
      case 'suggestion':
        // Update suggestions UI
        setSuggestion(message.suggestion || '');
        break;
      
      case 'error':
        console.error('Server error:', message.error);
        toast({
          title: "Error",
          description: message.error || "An unknown error occurred",
          variant: "destructive"
        });
        break;

      default:
        // Handle transcript field directly if present
        if (message.transcript) {
          setTranscript(prev => prev + message.transcript + '\n');
        }
        
        // Handle suggestion field directly if present
        if (message.suggestion) {
          setSuggestion(message.suggestion);
        }
        break;
    }
  }, [toast]);

  const handleStartRecording = async (setDebugLog: React.Dispatch<React.SetStateAction<string[]>>, chatId: string | null) => {
    console.log('handleStartRecording called');
    
    try {
      // Request both audio and screen capture permissions
      const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const displayStream = await navigator.mediaDevices.getDisplayMedia({ 
        video: { 
          displaySurface: "monitor" 
        },
        audio: true 
      });
      
      // If we get here, permissions were granted
      setIsRecording(true);
      addDebugLog('Permissions granted. Recording started.', setDebugLog);
      
      // Connect to WebSocket if we have a chatId
      if (chatId) {
        const ws = getWebSocketInstance(chatId);
        ws.connect();
        
        // Register message handler
        const removeMessageHandler = ws.onMessage(handleWebSocketMessage);
        
        addDebugLog('WebSocket connection initiated', setDebugLog);
        
        // Clean up function to remove message handler on unmount
        return () => {
          removeMessageHandler();
          audioStream.getTracks().forEach(track => track.stop());
          displayStream.getTracks().forEach(track => track.stop());
        };
      }
      
      // Clean up function to stop tracks when recording is stopped
      return () => {
        audioStream.getTracks().forEach(track => track.stop());
        displayStream.getTracks().forEach(track => track.stop());
      };
    } catch (error) {
      console.error('Error getting media permissions:', error);
      addDebugLog(`Permission error: ${error}`, setDebugLog);
      toast({
        title: "Permission Error",
        description: "Could not access microphone or screen. Please grant permissions and try again.",
        variant: "destructive"
      });
    }
  };

  const handleStopRecording = (setDebugLog: React.Dispatch<React.SetStateAction<string[]>>) => {
    console.log('handleStopRecording called');
    setIsRecording(false);
    addDebugLog('Recording stopped', setDebugLog);
    
    // Close WebSocket connection when recording is stopped
    closeWebSocketInstance();
    addDebugLog('WebSocket connection closed', setDebugLog);
    
    toast({
      description: "Recording stopped",
    });
  };

  const handleSuggest = (setDebugLog: React.Dispatch<React.SetStateAction<string[]>>) => {
    console.log('handleSuggest called');
    addDebugLog('Suggestion requested', setDebugLog);
    toast({
      description: "Requesting suggestion...",
    });
    
    // Send message to request suggestion if WebSocket is connected
    const ws = getWebSocketInstance(""); // We already have a singleton instance
    if (ws.isConnected()) {
      ws.sendMessage({
        type: "request_suggestion",
        content: { transcript }
      });
    } else {
      // Fallback for when WebSocket is not connected
      setTimeout(() => {
        setSuggestion('This is a sample suggestion based on your transcript. In a real application, this would be generated by an AI based on the recorded speech.');
      }, 1000);
    }
  };

  const handleResetContext = (setDebugLog: React.Dispatch<React.SetStateAction<string[]>>) => {
    console.log('handleResetContext called');
    setTranscript('');
    setSuggestion('');
    addDebugLog('Context reset', setDebugLog);
    toast({
      description: 'Context has been reset',
    });
  };

  return {
    isRecording,
    transcript,
    suggestion,
    isConnected,
    handleStartRecording,
    handleStopRecording,
    handleSuggest,
    handleResetContext,
    setTranscript,
    setSuggestion,
    setIsConnected
  };
};
